import os
import torch
import unittest

from dataset import FofeDataset
from embedding import NgramHashing, NgramEmbedding


class TestDataset(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        """ This is called before tests in an individual class are run. """
        with open("test_word_hashing.csv", "w") as text_file:
            text_file.write("Verdun\nPie-IX")

        with open("test_dataset.csv", "w") as text_file:
            text_file.write("EntityMention,EntityClass\nVerdun,0\nPie-IX,1")
        cls.vocab_length = 15   # verdun + pix-ix --> ngram=2 --> vocab of 15 differents ngram

        cls.dataset = FofeDataset("test_dataset.csv", "EntityMention", "EntityClass", "utf-8",
                                  number_of_classes=2, ngram=2, forgetting_factor=0.5)

    @classmethod
    def tearDownClass(cls):
        """ A class method called after tests in an individual class have run. """
        try:
            # delete files generated by the prior tests
            for filename in ['test_dataset.csv', 'test_word_hashing.csv',
                             'test_embedding.pth', 'test_vocab.pickle']:
                if os.path.isfile(filename):
                    os.remove(filename)
        except OSError as oserr:
            print(oserr)

    def setUp(self):
        """ This is called immediately before calling the test method. """
        pass

    def test_dataset_length(self):
        self.assertEqual(2, len(self.dataset))

    def test_vocab_oov(self):
        # test vocab created
        self.assertGreater(len(self.dataset.vocab), 0)
        self.assertEqual(self.dataset.vocab[0], '##')

    def test_vocab(self):
        self.assertEqual(len(self.dataset.vocab), self.vocab_length)
        self.assertEqual(self.dataset.vocab,
                         ['##', '#v', 've', 'er', 'rd', 'du', 'un', 'n#', '#p', 'pi', 'ie', 'e-', '-i', 'ix', 'x#'])

    def test_get_item_basic(self):
        x0, y0 = self.dataset[0]
        x1, y1 = self.dataset[1]

        # input x have the same length that vocab size
        self.assertEqual(self.vocab_length, len(x0))
        self.assertEqual(self.vocab_length, len(x1))

        self.assertEqual(0, y0)
        self.assertEqual(1, y1)

    def test_get_item_fofe_encoding(self):
        x0, _ = self.dataset[0]

        ff = self.dataset.fofe.forgetting_factor

        # x0 is the fofe encoding of 'Verdun'
        # 'Verdun' indices in vocab are [1,2,3,4,5,6,7]
        # 'Verdun' FOFE encoding (vocab size of 13, forgetting factor = 0.5):
        #     [0.0000, 0.0156, 0.0312, 0.0625, 0.1250, 0.2500, 0.5000, 1.0000,
        #      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]
        expected_fofe_encoding = torch.tensor(
            [0, ff**6, ff**5, ff**4, ff**3, ff**2, ff**1, ff**0,
             0, 0, 0, 0, 0, 0, 0],
            dtype=torch.float32)

        self.assertTrue(expected_fofe_encoding.equal(x0))

    def test_one_hot(self):
        test_bigram = NgramHashing(2, "test_word_hashing.csv")

        dset = FofeDataset("test_dataset.csv", "EntityMention", "EntityClass", "utf-8",
                           number_of_classes=2, forgetting_factor=0.5)
        x1, y1 = dset[1]

        # input x have the same length that vocab size
        self.assertEqual(self.vocab_length, len(x1))
        print(f"fofe onehot\n{x1}")

        self.assertEqual(1, y1)

    def test_embedding(self):
        embedding_dim = 8
        test_bigram = NgramHashing(2, "test_word_hashing.csv")

        # create and train an embedding
        training_sentence = test_bigram.ngram('Verdun') + test_bigram.ngram('Pie-IX')
        embedding = NgramEmbedding(ngram=test_bigram)
        embedding.train_ngram(training_sentence, 100, embedding_dim=embedding_dim, verbose=False)
        embedding.save('test_embedding.pth', 'test_vocab.pickle')

        dset = FofeDataset("test_dataset.csv", "EntityMention", "EntityClass", "utf-8",
                           embedding=embedding, number_of_classes=2, forgetting_factor=0.5)
        x1, y1 = dset[1]

        # input x have the same length that embedding dim
        self.assertEqual(embedding_dim, len(x1))
        print(f"fofe embedding\n{x1}")

        self.assertEqual(1, y1)


if __name__ == '__main__':
    unittest.main()
